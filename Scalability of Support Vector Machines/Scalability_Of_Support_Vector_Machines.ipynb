{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K__IYW7MeeL"
      },
      "source": [
        "Task 1 \\\\\n",
        "Implement a Python class named LinearSVC which learns a linear Support Vector Classifier (SVC) from a set of training data. The class is required to have the following components: \\\\\n",
        "\n",
        "\n",
        "*    A constructor init which initialize an SVC using the given learning rate, number of\n",
        "epochs and a random seed. (Similar to the perceptron class in our textbook.)\n",
        "*   A training function fit which trains the SVC based on a given labeled dataset. We consider\n",
        "the soft-margin SVC using a hinge loss. You are required to integrate L2-regularization and expose the\n",
        "regularization parameter to users\n",
        "*   A function net input which computes the preactivation value for a given input sample.\n",
        "*   A function predict which generates the prediction for a given input sample.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUAflhNvQXBx"
      },
      "outputs": [],
      "source": [
        "import numpy as np;\n",
        "class LinearSVC:\n",
        "  \"\"\"\n",
        "  A constructor init which initialize an SVC using the given learning rate, number of\n",
        "  epochs and a random seed\n",
        "  \"\"\"\n",
        "  def __init__(self, eta=0.001, n_iter=50, random_state=1):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.random_state = random_state\n",
        "\n",
        "  \"\"\"\n",
        "  A training function fit which trains the SVC based on a given labeled dataset. We consider\n",
        "  the soft-margin SVC using a hinge loss. You are required to integrate L2-regularization and expose the\n",
        "  regularization parameter to users.\n",
        "  \"\"\"\n",
        "  \"\"\"Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_examples, n_features]\n",
        "          Training vectors, where n_examples is the number of\n",
        "          examples and n_features is the number of features.\n",
        "        y : array-like, shape = [n_examples]\n",
        "          Target values.\n",
        "        C : Regularization parameter. Should be greater than 0\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "  def fit(self, X, y, C=1):\n",
        "\n",
        "      rgen = np.random.RandomState(self.random_state)\n",
        "      self.w_ = rgen.normal(loc=0.0, scale=0.01,\n",
        "                            size=X.shape[1])\n",
        "      self.b_ = np.float_(0.)\n",
        "      self.errors_ = []\n",
        "\n",
        "      for _ in range(self.n_iter):\n",
        "          errors = 0\n",
        "          for xi, target in zip(X, y):\n",
        "\n",
        "              # Predicted score (wx+b)\n",
        "              predicted_score = np.dot(self.w_ ,xi) + self.b_\n",
        "\n",
        "              # Hinge Loss\n",
        "              hinge_loss = max(0, 1 - target * predicted_score)\n",
        "              if hinge_loss > 0:\n",
        "                # Gradients w.r.t weights and bias\n",
        "                dw = -target * xi + C * self.w_\n",
        "                db = -target\n",
        "                errors += 1\n",
        "              else:\n",
        "                dw = C * self.w_\n",
        "                db = 0\n",
        "              # Updating weights with L2-regularization and bias\n",
        "              self.w_ -= self.eta * dw\n",
        "              self.b_ -= self.eta * db\n",
        "\n",
        "          self.errors_.append(errors)\n",
        "      return self\n",
        "\n",
        "  \"\"\"\n",
        "  A function net input which computes the preactivation value for a given input sample.\n",
        "  \"\"\"\n",
        "  def net_input(self, X):\n",
        "    \"\"\"Calculate net input\"\"\"\n",
        "    return np.dot(X, self.w_) + self.b_\n",
        "\n",
        "  \"\"\"\n",
        "  A function predict which generates the prediction for a given input sample.\n",
        "  \"\"\"\n",
        "  def predict(self, X):\n",
        "    \"\"\"Return class label after unit step\"\"\"\n",
        "    return np.where(self.net_input(X) >= 0.0, 1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eox1AFu407Nk"
      },
      "outputs": [],
      "source": [
        "import numpy as np;\n",
        "class LinearSVC:\n",
        "  \"\"\"\n",
        "  A constructor init which initialize an SVC using the given learning rate, number of\n",
        "  epochs and a random seed\n",
        "  \"\"\"\n",
        "  def __init__(self, eta=0.001, n_iter=50, random_state=1):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.random_state = random_state\n",
        "\n",
        "  \"\"\"\n",
        "  A training function fit which trains the SVC based on a given labeled dataset. We consider\n",
        "  the soft-margin SVC using a hinge loss. You are required to integrate L2-regularization and expose the\n",
        "  regularization parameter to users.\n",
        "  \"\"\"\n",
        "  \"\"\"Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_examples, n_features]\n",
        "          Training vectors, where n_examples is the number of\n",
        "          examples and n_features is the number of features.\n",
        "        y : array-like, shape = [n_examples]\n",
        "          Target values.\n",
        "        C : Regularization parameter. Should be greater than 0\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "  def fit(self, X, y, C=1):\n",
        "      n_samples = X.shape[0]\n",
        "      rgen = np.random.RandomState(self.random_state)\n",
        "      self.w_ = rgen.normal(loc=0.0, scale=0.01,\n",
        "                            size=X.shape[1])\n",
        "      self.b_ = np.float_(0.)\n",
        "      self.errors_ = []\n",
        "\n",
        "      for _ in range(self.n_iter):\n",
        "          errors = 0\n",
        "          for xi, target in zip(X, y):\n",
        "\n",
        "              # Margin (y'*(wx+b))\n",
        "              margin = target * (np.dot(self.w_ ,xi) + self.b_)\n",
        "\n",
        "              # Hinge Loss\n",
        "              # hinge_loss = max(0, 1 - margin)\n",
        "              if margin < 1:\n",
        "                # Gradients w.r.t weights and bias\n",
        "                dw = self.w_/n_samples - C * target * xi\n",
        "                db = -C * target\n",
        "                errors += 1\n",
        "              else:\n",
        "                dw = self.w_ / n_samples\n",
        "                db = 0.0\n",
        "              # Updating weights and bias\n",
        "              self.w_ -= self.eta * dw\n",
        "              self.b_ -= self.eta * db\n",
        "          self.errors_.append(errors)\n",
        "      return self\n",
        "\n",
        "  \"\"\"\n",
        "  A function net input which computes the preactivation value for a given input sample.\n",
        "  \"\"\"\n",
        "  def net_input(self, X):\n",
        "    \"\"\"Calculate net input\"\"\"\n",
        "    return np.dot(X, self.w_) + self.b_\n",
        "\n",
        "  \"\"\"\n",
        "  A function predict which generates the prediction for a given input sample.\n",
        "  \"\"\"\n",
        "  def predict(self, X):\n",
        "    \"\"\"Return class label after unit step\"\"\"\n",
        "    return np.where(self.net_input(X) >= 0.0, 1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnM1vK0MMJbf"
      },
      "source": [
        "Task 2 \\\\\n",
        "Write a Python function make classification which generates a set of linearly separable data\n",
        "based on a random separation hyperplane. We learned that an (d − 1)-dimensional hyperplane can be defined\n",
        "as the set of points in the Euclidean space Rd satisfying an equation  ̄aT  ̄x = b, i.e., { ̄x ∈ Rd |  ̄aT  ̄x = b}. For\n",
        "simplicity, we assume that b = 0, then the hyperplane can be determined by a random vector  ̄a. We use this\n",
        "idea to design the following algorithm to generate random data which are linearly separable:\n",
        "\n",
        "*   Step 1. Randomly generate a d-dimensional vector  ̄a.\n",
        "*   Step 2. Randomly select n samples  ̄x1, . . . ,  ̄xn in the range of [−u, u] in each dimension. You may use a uniform or Gaussian distribution to do so.\n",
        "*   Step 3. Give each  ̄xi a label yi such that if  ̄aT  ̄x < 0 then yi = −1, otherwise yi = 1.\n",
        "\n",
        "Therefore, your function should have the following parameters that should given by the user: d, n, u, and a\n",
        "random seed for reproducing the data. You need to additionally subdivide the dataset to a training dataset\n",
        "(70%) and a test dataset (30%). You may use the scikit-learn function to do so, but make sure that you\n",
        "specify the random seed such that the subdivision is reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwUbXhLYL7rU",
        "outputId": "8d1dc886-a758-4790-f9a0-b173cd5244fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported Dataset\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def make_classification(n, d, u=1.0, random_seed=1):\n",
        "    \"\"\"\n",
        "    Generates a set of linearly separable data based on a random separation hyperplane.\n",
        "\n",
        "    Parameters:\n",
        "    - n: Number of samples to generate.\n",
        "    - d: Dimensionality of the data.\n",
        "    - u: Range for generating random samples in each dimension (default is 1.0).\n",
        "    - random_seed: Seed for random number generation (optional).\n",
        "\n",
        "    Returns:\n",
        "    - X: A numpy array of shape (n, d) containing the generated samples.\n",
        "    - y: A numpy array of shape (n,) containing the labels (-1 or 1) for each sample.\n",
        "    - a: The random vector defining the separation hyperplane.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Step 1: Randomly generate a d-dimensional vector a\n",
        "    a = np.random.randn(d)\n",
        "\n",
        "    # Step 2: Randomly select n samples in the range [-u, u] in each dimension\n",
        "    X = np.random.uniform(low=-u, high=u, size=(n, d))\n",
        "\n",
        "    # Step 3: Assign labels based on the position relative to the hyperplane\n",
        "    y = np.where(np.dot(X, a) < 0, -1, 1)\n",
        "\n",
        "    # Split into training(70%) and testing data(30%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "    return X_test, X_train, y_train, y_test, X, y\n",
        "\n",
        "# Example usage:\n",
        "n = 100  # Number of samples\n",
        "d = 2    # Dimensionality\n",
        "u = 5.0  # Range for generating samples\n",
        "\n",
        "X_test, X_train, y_train, y_test,X, y = make_classification(n, d, u, random_seed=1)\n",
        "\n",
        "def export_data_numpy(X_train, X_test, y_train, y_test, filename=\"dataset\"):\n",
        "    np.savetxt(f\"{filename}_X_train.csv\", X_train, delimiter=\",\")\n",
        "    np.savetxt(f\"{filename}_X_test.csv\", X_test, delimiter=\",\")\n",
        "    np.savetxt(f\"{filename}_y_train.csv\", y_train, delimiter=\",\")\n",
        "    np.savetxt(f\"{filename}_y_test.csv\", y_test, delimiter=\",\")\n",
        "\n",
        "    # Export full dataset (X, y)\n",
        "    np.savetxt(f\"{filename}_X_full.csv\", X, delimiter=\",\")\n",
        "    np.savetxt(f\"{filename}_y_full.csv\", y, delimiter=\",\")\n",
        "\n",
        "    print('Exported Dataset')\n",
        "\n",
        "# Example usage:\n",
        "export_data_numpy(X_train, X_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfvSOF4Id4QW"
      },
      "source": [
        "Task 3 \\\\\n",
        "Investigate the scalability of the LinearSVC class you have implemented. You may consider the\n",
        "datasets of the combinations of the following scales: d = 10, 50, 100, 500, 1000 and n = 500, 1000, 5000,\n",
        "10000, 100000. Please feel free to adjust the scales according to your computers’ configurations, however the time costs should be obviously different. Make sure that you use the same dataset for each combination.\n",
        "This can be controlled by using the same random seed (see textbook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "efbeTjxBeGNv",
        "outputId": "34e872b6-c283-450d-cb80-9ab75ad68884"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Cannot load file containing pickled data when allow_pickle=False",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-14db76eecdee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Generate a linearly separable dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dataset_X_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dataset_X_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dataset_y_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[1;32m    463\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Define the combinations of n (samples) and d (features)\n",
        "n_values = [500, 1000, 5000, 10000, 100000]\n",
        "d_values = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Iterate over all combinations of n and d\n",
        "for nv in n_values:\n",
        "    for dv in d_values:\n",
        "        # Generate a linearly separable dataset\n",
        "        X_train = np.load(f\"dataset_X_train.csv\")\n",
        "        X_test = np.load(f\"dataset_X_test.csv\")\n",
        "        y_train = np.load(f\"dataset_y_train.csv\")\n",
        "        y_test = np.load(f\"dataset_y_test.csv\")\n",
        "\n",
        "        # Initialize LinearSVC\n",
        "        model = LinearSVC(eta=0.01, n_iter=10000, random_state=1)\n",
        "\n",
        "        # Measure training time\n",
        "        start_time = time.time()\n",
        "        model.fit(X, y, C=1)\n",
        "        end_time = time.time()\n",
        "\n",
        "        execution_time = end_time - start_time\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = np.mean(y_pred == y_test)\n",
        "\n",
        "        # Store results\n",
        "        results.append((nv, dv, execution_time))\n",
        "\n",
        "# Print results in a table\n",
        "print(\"\\nResults:\")\n",
        "print(\"n\\t\\td\\t\\tTraining Time (s)\")\n",
        "print(\"-----------------------------------------\")\n",
        "for result in results:\n",
        "    print(f\"{result[0]}\\t\\t{result[1]}\\t\\t{result[2]:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
